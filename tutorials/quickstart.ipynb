{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for x, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using cross entropy here, others are available\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using stochastic gradient descent, others are available\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training will take in the dataloader, model, loss function, and an optimizer. The training method will loop through the batches in the dataloader and complete forward passes through the model followed by backpropogation to adjust weights and biases accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation will take in the dataloader, model, and loss function in order to validate against the now trained model to evaluate performance. This does not take in an optimizer because no optimization is going to be done during validation. This is why `with torch.no_grad()` is used to relay to torch that gradients will not be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.293347  [   64/60000]\n",
      "loss: 2.284591  [ 6464/60000]\n",
      "loss: 2.271797  [12864/60000]\n",
      "loss: 2.267611  [19264/60000]\n",
      "loss: 2.241725  [25664/60000]\n",
      "loss: 2.220431  [32064/60000]\n",
      "loss: 2.226617  [38464/60000]\n",
      "loss: 2.190961  [44864/60000]\n",
      "loss: 2.187884  [51264/60000]\n",
      "loss: 2.161678  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 2.153034 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.160940  [   64/60000]\n",
      "loss: 2.149451  [ 6464/60000]\n",
      "loss: 2.100042  [12864/60000]\n",
      "loss: 2.111482  [19264/60000]\n",
      "loss: 2.057364  [25664/60000]\n",
      "loss: 2.007777  [32064/60000]\n",
      "loss: 2.023328  [38464/60000]\n",
      "loss: 1.952967  [44864/60000]\n",
      "loss: 1.959490  [51264/60000]\n",
      "loss: 1.880557  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.882261 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.919310  [   64/60000]\n",
      "loss: 1.882825  [ 6464/60000]\n",
      "loss: 1.778069  [12864/60000]\n",
      "loss: 1.804026  [19264/60000]\n",
      "loss: 1.693349  [25664/60000]\n",
      "loss: 1.663537  [32064/60000]\n",
      "loss: 1.664085  [38464/60000]\n",
      "loss: 1.584466  [44864/60000]\n",
      "loss: 1.612039  [51264/60000]\n",
      "loss: 1.493982  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.518790 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.592896  [   64/60000]\n",
      "loss: 1.548960  [ 6464/60000]\n",
      "loss: 1.411560  [12864/60000]\n",
      "loss: 1.466215  [19264/60000]\n",
      "loss: 1.349928  [25664/60000]\n",
      "loss: 1.363142  [32064/60000]\n",
      "loss: 1.358367  [38464/60000]\n",
      "loss: 1.296725  [44864/60000]\n",
      "loss: 1.333056  [51264/60000]\n",
      "loss: 1.226466  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.255744 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.337029  [   64/60000]\n",
      "loss: 1.311918  [ 6464/60000]\n",
      "loss: 1.152027  [12864/60000]\n",
      "loss: 1.248538  [19264/60000]\n",
      "loss: 1.123879  [25664/60000]\n",
      "loss: 1.161027  [32064/60000]\n",
      "loss: 1.170121  [38464/60000]\n",
      "loss: 1.114294  [44864/60000]\n",
      "loss: 1.153897  [51264/60000]\n",
      "loss: 1.068766  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.090308 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Epochs are the number of times to go through the training and testing loop\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to saved_models/quickstart_model.pth\n",
      "Loaded PyTorch Model State from saved_models/quickstart_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"saved_models/quickstart_model.pth\"\n",
    "\n",
    "# Saving Model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Saved PyTorch Model State to {model_path}\")\n",
    "\n",
    "# Loading Model\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(f\"Loaded PyTorch Model State from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "# These classes are dataset dependant\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "# Using the model loaded in the previous step\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "# Bypassing gradient similar to validation\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
